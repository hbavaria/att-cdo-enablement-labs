{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Notebook for native spark app (wx.data SaaS) submission via UI\n",
    "The main steps include:\n",
    "- `spark-processing.py` script that contains spark app is copied to COS bucket connected to iceberg catalog {WXD_BUCKET}\n",
    "- we prepare JSON input file that needs to be provided for app submission in watsonx.data UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set files with env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caedfd1f-4d0c-42e1-967a-166669af2c48"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from base64 import b64encode\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import ClientError, Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea6e2543-f2af-472c-9438-a98e137bcb7c"
   },
   "source": [
    "## Load env.txt file with configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f6eac24-4213-48f3-8b63-1a970b78c09e"
   },
   "outputs": [],
   "source": [
    "with open('.env_all', 'wb') as env_file:\n",
    "    env_file.write(wslib.load_data('env.txt').read())\n",
    "# environmental variables store credentials and configuration\n",
    "load_dotenv('.env_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading credentials and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f1970e6-f7ac-496b-b640-680a3e71f71d"
   },
   "outputs": [],
   "source": [
    "CLOUD_API = getpass.getpass(\"Enter watsonx.data backend Cloud API key: \")\n",
    "print(\"Cloud API Key received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56e9c928-f57c-4829-9e2b-3f200b7159f7"
   },
   "outputs": [],
   "source": [
    "cos_conn = wslib.get_connection('ATT_Enablement_cos_connection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aa80af1-34e4-4d9a-9fb3-f1f99a741f9f"
   },
   "outputs": [],
   "source": [
    "# spark script locally\n",
    "spark_script_name = 'spark-processing.py'\n",
    "\n",
    "# spark scrit in COS bucket location\n",
    "spark_script_path = f'spark-scripts/{spark_script_name}'\n",
    "\n",
    "# wx.data credentials\n",
    "WXD_USER = \"ibmlhapikey\"\n",
    "CLOUD_USER_ID = os.getenv(\"CLOUD_USER_ID\")\n",
    "\n",
    "# connected COS bucket name\n",
    "cos_bucket_name = os.getenv(\"WXD_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "366c2a66-e055-45df-aa00-4404f6a08f81"
   },
   "outputs": [],
   "source": [
    "# Constants for IBM COS values\n",
    "COS_ENDPOINT = f\"https://s3.ca-tor.cloud-object-storage.appdomain.cloud\"\n",
    "COS_API_KEY_ID = cos_conn['api_key']\n",
    "COS_INSTANCE_CRN = cos_conn['resource_instance_id']\n",
    "HIVE_BUCKET = os.environ[\"HIVE_BUCKET\"]\n",
    "WXD_BUCKET = os.environ[\"WXD_BUCKET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure that COS_ENDPOINT is the same as for buckets and the same for all buckets, if not -> replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(COS_ENDPOINT)\n",
    "# COS_ENDPOINT = \"enter https-prepended endpoint and uncomment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "client = ibm_boto3.client(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b39e80b-3741-4b3b-a404-723cfbbab602"
   },
   "source": [
    "#### Encoded api string for payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "600765c3-6ea8-4383-8fd3-b0ce71c8e9ee"
   },
   "outputs": [],
   "source": [
    "bytes_string = f\"{WXD_USER}_{CLOUD_USER_ID}:{CLOUD_API}\".encode('utf-8')\n",
    "base64_bytes = b64encode(bytes_string)\n",
    "base64_string = base64_bytes.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload spark app script to COS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0386140b-44e3-4d6e-b94e-f7a7694c7e81"
   },
   "outputs": [],
   "source": [
    "# download py file locally\n",
    "with open(f'./{spark_script_name}', 'wb') as env_file:\n",
    "    env_file.write(wslib.load_data(spark_script_name).read())\n",
    "# upload py file to watsonx.data bucket\n",
    "client.upload_file(f'./{spark_script_name}', cos_bucket_name, spark_script_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate payload for spark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91cc1ea6-0338-4b1a-b1fd-e4417b579312"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"application_details\": {\n",
    "        \"conf\": {\n",
    "            \"spark.hadoop.wxd.apikey\": f\"Basic {base64_string}\",\n",
    "            \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"spark.myenv.hive_bucket\": os.getenv(\"HIVE_BUCKET\"),\n",
    "            \"spark.myenv.iceberg_catalog\": os.getenv(\"ICEBERG_CATALOG\"),\n",
    "            \"spark.myenv.hive_catalog\": \"spark_catalog\",\n",
    "            \"spark.myenv.schema_data_hive\": os.getenv(\"SCHEMA_DATA_H\"),\n",
    "            \"spark.myenv.schema_data_iceberg\": os.getenv(\"SCHEMA_DATA_I\"),\n",
    "            \"spark.myenv.schema_netezza_offload\": os.getenv(\"SCHEMA_DWH_OFFLOAD\"),\n",
    "    },\n",
    "        \"application\": f\"s3a://{cos_bucket_name}/{spark_script_path}\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef754ee6-8c6b-487e-ad5c-42ed58966f89"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(data, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
